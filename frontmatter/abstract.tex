%!TEX root = ../dissertation.tex

Collaborative perception systems help autonomous vehicles see more of their surroundings by sharing information with nearby agents. This teamwork improves awareness and planning, but it also brings new risks: mistakes made by one vehicle can spread to others. 
In this work, we explore the security challenges in collaborative multi-agent perception. We focus on physical adversarial patch attacks, where an outside attacker with limited knowledge tries to fool the system. Our study considers realistic situations, including both stationary and moving attacks, and looks at how both directly targeted vehicles and others in the network can be affected.
Through analysis and simulation, we show that a single manipulated input can cause failures across the whole system. Our findings underline the need for robust defenses in collaborative perception and offer guidance for building safer, more resilient systems.
