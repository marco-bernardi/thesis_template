%!TEX root = ../dissertation.tex
\chapter{Introduction}
\label{introduction}

\section{Autonomous and Semi-Autonomous Vehicles}
Autonomous vehicles (AVs) are revolutionizing transportation, with both fully self-driving and semi-autonomous cars gaining attention from researchers and automakers for their potential to create safer, more efficient roads.
Modern vehicles come equipped with advanced driver-assistance systems and AI perception that help them spot obstacles, understand traffic signals, and navigate independently.
In theory, these vehicles can outperform human drivers by reacting faster without fatigue or distraction, potentially reducing accident rates significantly.
However, the reality shows these systems still have flaws. Self-driving cars sometimes make perception errors that lead to incorrect decisions with serious consequences.
Take the widely reported fatal crash where an autonomous vehicle detected a highway divider but mistakenly classified it as non-threatening, contributing to the accident.
Such incidents underscore the critical importance of reliable perception, as autonomous vehicles must accurately interpret their surroundings to ensure safety.
Yet, single-vehicle perception systems have inherent limitations, highlighting the need for more robust approaches such as collaborative perception.

\section{SAE Levels of Driving Automation}

The Society of Automotive Engineers (SAE) has established a widely adopted taxonomy to classify the progressive stages of driving automation. This taxonomy, codified in SAE J3016, is a descriptive convention that standardizes terminology across engineering, policy, and legal discourse. It distinguishes between driver support features and automated driving system (ADS) features, clarifies the allocation of roles for the human user and the ADS in performing the dynamic driving task (DDT), and provides a common reference for comparing capabilities and limitations \cite{SAEJ3016_2021}.

\paragraph{Scope and terminology.}
SAE levels apply to the engaged feature or features at a given time rather than to a vehicle model in the abstract. Levels 0 through 2 are driver support features where the human user monitors the driving environment. Levels 3 through 5 are ADS features that perform the entire DDT within an operational design domain, with differing responsibilities for fallback and minimal risk conditions \cite{SAEJ3016_2021}.

\begin{enumerate}
    \item \textbf{Level 0: No Driving Automation.}
    The human driver performs the entire DDT at all times. Active safety systems may provide momentary interventions, yet no sustained lateral or longitudinal control is automated \cite{SAEJ3016_2021}.

    \item \textbf{Level 1: Driver Assistance.}
    A driver support feature provides continuous assistance in either lateral control or longitudinal control. The human driver monitors the environment and executes the remainder of the DDT \cite{SAEJ3016_2021}.

    \item \textbf{Level 2: Partial Driving Automation.}
    A driver support feature provides sustained assistance in both lateral and longitudinal control under specified conditions. The human driver continuously supervises the driving environment and remains responsible for immediate intervention \cite{SAEJ3016_2021}.

    \item \textbf{Level 3: Conditional Driving Automation.}
    An ADS performs the entire DDT within a defined operational design domain, including environmental monitoring. The human user is not required to monitor the environment but must be available to resume control upon a takeover request \cite{SAEJ3016_2021}.

    \item \textbf{Level 4: High Driving Automation.}
    An ADS performs the entire DDT within its operational design domain without expecting human intervention and achieves a minimal risk condition if it cannot continue. No human fallback is required within the domain \cite{SAEJ3016_2021}.

    \item \textbf{Level 5: Full Driving Automation.}
    An ADS performs the entire DDT under all roadway and environmental conditions that a competent human driver could manage. No human driver is required at any time \cite{SAEJ3016_2021}.
\end{enumerate}

\section{Collaborative Perception in Connected Vehicles}
Individual vehicles face perception constraints from their sensors' limited field of view and range. Two major challenges stand out: occlusion (when objects are blocked from view) and restricted sensor range. Occlusion happens constantly in traffic, as a pedestrian might be hidden behind another vehicle, invisible to an approaching autonomous car's cameras or LiDAR.
In traditional autonomous driving, the car wouldn't detect this hidden hazard until potentially too late to avoid collision. Collaborative perception offers a promising solution: when vehicles communicate with each other, they can share what they see, essentially allowing them to "see around corners."
For example, one car might clearly view a pedestrian that's completely blocked from another car's sight; by sharing this information, the second vehicle receives advance warning and can respond appropriately. Similarly, vehicles can help extend each other's sensing capabilities beyond their natural limits.
While a car's LiDAR typically only detects objects within about 90 meters, a vehicle further ahead can provide information about traffic conditions beyond that range, giving advance notice of hazards that would otherwise remain undetected until much closer.
To enable such cooperation, autonomous vehicles are increasingly being equipped with wireless vehicle-to-vehicle (V2V) and vehicle-to-everything (V2X) communication capabilities. This connectivity transforms isolated self-driving cars into a network of connected autonomous vehicles (CAVs) that can exchange real-time information about their surroundings.
Collaborative perception frameworks allow multiple CAVs to fuse their sensor data (camera images, LiDAR point clouds, radar readings, etc.) in order to build a much more comprehensive understanding of the environment than any single vehicle could on its own.
Research prototypes have demonstrated that sharing sensor observations or even intermediate features between cars significantly improves the accuracy of detecting objects and predicting hazards, thereby enhancing overall driving safety.
Different strategies exist for how information is shared and combined, ranging from early fusion (transmitting raw sensor data to neighbors for joint processing) to late fusion (each vehicle sharing only high-level detected objects or intents), each balancing communication bandwidth against perceptual gain.
Regardless of the method, collaborative perception is widely seen as a promising approach to overcome single-vehicle limitations, reduce occlusion dangers, and generally make autonomous driving more reliable.
It also aligns with the broader trend of connected smart transportation, where cars cooperate with each other and with infrastructure (e.g. smart traffic lights or roadside sensors) to improve traffic flow and safety.
However, realizing robust collaborative autonomous driving in the real world comes with numerous challenges beyond just getting vehicles to talk to each other. Practical deployment must address issues of data format standardization, network latency, privacy and trust among vehicles, and ensuring the communication overhead does not outweigh the benefits.
In particular, trust and security in the shared data are paramount; if one vehicle provides erroneous or malicious information, it could mislead others. Thus, alongside technical integration, researchers must also confront the security vulnerabilities that emerge in multi-vehicle collaboration.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/introduction/permission-settings.png}
    \caption{Collaborative perception in connected autonomous vehicles}
    \label{fig:permission-settings}
\end{figure}

As shown in \autoref{fig:permission-settings}, collaborative perception helps cars overcome the limits of their own sensors.
On the left, the blue car can’t see a pedestrian crossing the street because a red vehicle blocks its view. But a white car coming from the other direction spots the pedestrian clearly and shares that information. Thanks to this, the blue car can react in time and avoid an accident.
On the right, the blue car is about to switch lanes but doesn’t see a fast-approaching vehicle that’s just outside its rear sensor range. Fortunately, a white car behind it picks up the approaching vehicle with its sensors and shares that data. With this extra input, the blue car gains a better picture of its surroundings and can change lanes more safely \cite{hu2025collaborativeperceptionconnectedautonomous}.

\section{Security Challenges and Adversarial Attacks in Collaborative Perception}

Connecting vehicles into a collaborative perception network significantly enhances their situational awareness, but it also introduces new security vulnerabilities.
In a standalone autonomous vehicle, an attacker would need to compromise that vehicle’s sensors or algorithms in isolation.
In contrast, a network of connected autonomous vehicles (CAVs) provides a broader attack surface, offering more opportunities for a malicious actor to inject false data that could be trusted and acted upon by multiple vehicles.
Collaborative perception systems are inherently more exposed to adversarial interference. Vehicles in a cooperative network often run the same perception models and exchange a range of data, including camera images, object detections, and neural network feature maps.
This shared data helps vehicles agree on what is around them. However, it also means that a compromised input from a single vehicle can influence the perception of the entire group.
For instance, an attacker might intercept vehicle-to-vehicle (V2V) communications and alter the data being transmitted. Alternatively, an adversary could gain control of one vehicle in the network and use it to broadcast deliberately deceptive observations.
Because this information is frequently encoded in machine-readable formats, such as feature tensors produced by deep learning models, subtle manipulations may not be immediately detectable by human operators or simple plausibility checks.
The result is that a skilled attacker can introduce false but convincing data into the system. The other vehicles, unaware of the deception, may accept the misleading input as valid and make driving decisions based on it.
This kind of vulnerability highlights a core challenge in collaborative autonomy: while shared perception improves awareness, it also requires robust safeguards to ensure the trustworthiness of every contributing node in the network.

A significant class of threats in the collaborative driving context involves adversarial attacks targeting the perception system. In these attacks, an adversary deliberately introduces subtle perturbations designed to mislead a vehicle’s sensors or machine learning models.
Prior research has demonstrated that even advanced vision systems, including object detectors and traffic sign classifiers, can be deceived using physical adversarial patches \cite{pmlr-v80-athalye18b} \cite{brown2018adversarialpatch} \cite{Chen_2019} \cite{goodfellow2015explainingharnessingadversarialexamples} \cite{szegedy2014intriguingpropertiesneuralnetworks} \cite{10.11453319535.3354259}.
These patches often take the form of printed stickers or patterns applied to stop signs, lane markers, or other features in the driving environment.
To a human observer, such patches may appear as harmless graffiti or decorative elements. However, when processed by a vehicle’s camera and neural network, they can cause severe misinterpretations.
For instance, a stop sign altered with an adversarial pattern might be classified as a speed limit sign or not recognized at all.
These kinds of physical adversarial examples have been validated both in controlled experiments and in real-world scenarios, where they have consistently caused perception systems to fail, despite the fact that the scene appears normal to the human eye.
In the context of autonomous driving, the implications of such attacks are clear. If a vehicle is tricked into perceiving a nonexistent green light or fails to detect a pedestrian, the resulting decisions can pose serious safety risks. These findings underscore the need for robust defense mechanisms that can detect or mitigate adversarial manipulations before they influence critical driving behavior \cite{zhu2023tpatchtriggeredphysicaladversarial}.

In collaborative perception systems, adversarial attacks can have consequences that extend far beyond a single vehicle. For example, an attacker might place a malicious patch on an object by the roadside. If one vehicle’s camera is deceived by the patch and produces a distorted view of the environment such as detecting a false hazard or failing to recognize a real one that misinformation may be shared with other nearby vehicles through the collaborative network. These vehicles, trusting the shared input, might then make incorrect driving decisions, even if their own sensors did not observe the manipulated object.

In this way, a single tampered input can trigger failures across the entire system. The collaborative network effectively amplifies and spreads the error, making it far more dangerous than if the vehicle operated in isolation. Recent studies on collaborative perception confirm this increased vulnerability. \cite{10.1007978-3-030-58536-5_36} \cite{96294f63afd44ccd9210824e1aa2656c} \cite{hu2022where2commcommunicationefficientcollaborativeperception} \cite{6a619c52e2df4c96ac051544a08e266d} When vehicles exchange feature maps or detection results, an attack on one agent can quietly poison the perception of the entire group, often without raising immediate alarms.

This kind of failure is particularly serious in autonomous driving, where false reports of obstacles or missed detections can result in unsafe behavior. In high-stakes traffic environments, the outcome might be a chain of incorrect decisions or even collisions involving multiple vehicles. The possibility of such cascading failures highlights the urgent need for strong security mechanisms in collaborative autonomous systems.

While collaborative perception offers significant potential to enhance the performance of autonomous vehicles, it also introduces important security challenges that must be addressed. The research community has begun to identify these risks and propose early-stage defenses, including methods for detecting malicious or inconsistent agents within the network \cite{10.1007978-3-030-58536-5_36} \cite{96294f63afd44ccd9210824e1aa2656c} \cite{hu2022where2commcommunicationefficientcollaborativeperception} \cite{6a619c52e2df4c96ac051544a08e266d}. However, more robust and comprehensive solutions are needed to ensure that connected autonomous vehicles (CAVs) can benefit from shared perception without being compromised by inaccurate or intentionally misleading data.

In this work, we focus on a specific threat: physical adversarial patch attacks in a multi-vehicle perception context. We examine how such an attack, even when carried out by an external adversary with limited knowledge of the system, can disrupt the integrity of collaborative perception. Our analysis underscores the need for secure and resilient collaborative frameworks and aims to support the development of defense mechanisms that can enable safer, more reliable autonomous vehicle networks.

\section{Research Questions}

Based on the current landscape and identified vulnerabilities in collaborative perception systems, this work is guided by the following research questions:
\begin{enumerate}
    \item How do vulnerabilities identified in single-vehicle perception models translate to or evolve within collaborative perception frameworks involving multiple vehicles?
    \item How do physical adversarial patch attacks scale in collaborative perception systems, and what new risks emerge when information is shared across multiple vehicles?
    \item How does the impact of a single compromised vehicle propagate through a collaborative network, and what are the consequences for group-level perception and decision-making?
    \item Which stages of the collaborative perception pipeline are most susceptible to adversarial interference introduced via physical patches? Azzardata nella nostra ricerca
    \item How can attack performance be effectively benchmarked across different collaborative fusion strategies, and what metrics best capture system resilience?
\end{enumerate}
These questions aim to guide a comprehensive analysis of both the threat landscape and the systemic weaknesses that adversarial patch attacks can exploit in multi-vehicle perception networks.

\section{Contributing}

Most V2X models have been trained and evaluated primarily on datasets collected either in simulated environments, such as those based on CARLA (for example, OPV2V\cite{xu2022opv2vopenbenchmarkdataset}), or in real-world scenarios, such as DAIR-V2X\cite{DAIR-V2X2021} and nuScenes\cite{nuscenes2019}.
However, there is limited literature examining their behaviour in real-time, real-world conditions under standard operating modes without adversarial interference.
Establishing this baseline is essential before assessing the feasibility of deploying adversarial attacks, as it provides a reference for understanding the models’ expected performance in non-perturbed scenarios.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/introduction/scale_car}
    \caption{Scaled model of the car used during testing, equipped with four cameras, two ArUco markers, and a proximity sensor.}
    \label{scale-car}
\end{figure}

\subsection{Static Testing}

Static testing refers to evaluating a model on a fixed set of collected data, without real-time interaction.
We performed two complementary static evaluations: the first on a closed road circuit with full control over vehicle positioning and environmental factors, and the second in an indoor setup using 1:20 scale vehicles and road infrastructure [see Figure \ref{scale-car}].
This approach allowed for highly controlled conditions, minimising external variability and providing an initial reference point for model behaviour.
Two collaborative perception models, CoBEVT\cite{xu2022cobevtcooperativebirdseye} and UniV2X\cite{yu2024_univ2x}, were selected for this phase to gain early practical insights into their capabilities.

\subsubsection{UniV2X}

UniV2X is an end-to-end cooperative autonomous driving framework integrating perception, mapping, occupancy prediction, and planning.
It leverages vehicle–infrastructure collaboration through sparse-dense hybrid fusion to maintain planning accuracy under communication constraints.
Despite its strong reported performance on DAIR-V2X, our preliminary observations indicated difficulties in accurately predicting object trajectories and detecting lane markings, suggesting a potential domain gap between training and test data.

\subsubsection{CoBEVT}

CoBEVT is a collaborative perception framework designed for bird’s eye view semantic segmentation in multi-agent, multi-camera settings.
It fuses data from multiple vehicles via the Fused Axial Attention module to mitigate occlusions and extend perception range.
Trained on OPV2V, CoBEVT has shown state-of-the-art performance in simulation, making it a relevant baseline for subsequent testing phases.

\subsection{Dynamic Testing}

Dynamic testing introduced real-time interaction between the models and the environment, with vehicles in motion and continuous sensor input.
Due to hardware and space constraints, these tests were conducted indoors using the same scale models and road setup from the static phase.
Vehicle positions and orientations were tracked in real time using two ArUco markers per vehicle, enabling continuous localisation data as input to the models.
While this setup introduced temporal dynamics absent in static tests, it remained limited in environmental diversity and scale.

\subsection{Simulator Testing}

While dynamic testing provided valuable insights, the constraints of our indoor setup and the limited scale of the experiments restricted the range of conditions we could explore.
In particular, hardware limitations and space constraints prevented us from replicating complex traffic scenarios, varying environmental conditions, or introducing a sufficient variety of cooperative agents.
To overcome these limitations and gain a more complete understanding of the models’ behaviour, we moved to a simulation-based evaluation.

Simulation offers a controlled and reproducible environment in which external factors can be systematically introduced, modified, and monitored.
This makes it possible to isolate specific variables and replicate identical scenarios across multiple runs, which is essential for evaluating model stability and repeatability.

For this stage of the study, we used CARLA\cite{Dosovitskiy17}, a widely recognised open-source simulator for autonomous driving research.
CARLA provides a high-fidelity urban environment with dynamic traffic participants, pedestrians, detailed infrastructure, and configurable weather and lighting conditions.
These features make it particularly well suited for testing collaborative perception models under both ideal and challenging scenarios.

An additional advantage of using CARLA is its direct connection to our models of interest.
It is the simulation engine used to generate the OPV2V dataset\cite{xu2022opv2vopenbenchmarkdataset}, which was employed for training CoBEVT.
This alignment between training and testing domains reduces part of the domain gap observed in our real-world evaluations, allowing a fairer assessment of the model’s nominal performance before introducing any adversarial perturbations.

By leveraging CARLA’s scenario control tools, we were able to vary parameters such as sensor placement, number and distribution of cooperating agents, occlusion patterns, and environmental conditions.
These experiments provided detailed insights into how each model processes shared visual information, how fusion strategies adapt to partial or noisy inputs, and how performance changes under controlled disturbances.
This simulation phase therefore served as a bridge between the highly variable real-world conditions and the idealised datasets used for training, highlighting the factors most critical to achieving operational robustness.

SCRIVERE CHE CALRA CI DA UNA GT IN MANIERA FACILE PER FARE CONFRONTO TRA IOU PATCHATO E NON?