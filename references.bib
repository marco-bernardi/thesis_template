
@BOOK{book1,
 title = {Title of the book with some {Upper Case LETTERS}},
 author = {Surname1, Name1 and Surname2, Name2 and Surname3, Name3},
 publisher = {Book publisher},
 edition = {1st},
 year = {2021}
}

@ARTICLE{article1,
  title = {Title of the article},
  author = {Surname4, Name4 and Surname5, Name5 and Surname6, Name6},
  journal = {Journal's name},
  volume = {1},
  number = {2},
  pages = {3--5},
  year = {2021}
}

@INPROCEEDINGS{conference1,
 title = {Title of the conference},
 author = {Surname7, Name7 and Surname8, Name8 and Surname9, Name9},
 booktitle = {Proceedings of the conference's name},
 month = {Feb.},
 year = {2021},
 address = {city, state}
}

@ONLINE{online1,
 author = {{Author of the resource}},
 title = {Title of the resource},
 url  = {https://unipd.it},
 addendum = {(accessed: 16.02.2021)}
}

@STANDARD{standard1,
 title = {Name of the standard},
 year = {2021},
 month = {Feb.},
 organization = {organization},
 note = {Identifier of the standard}
}




@misc{hu2025collaborativeperceptionconnectedautonomous,
  title         = {Collaborative Perception for Connected and Autonomous Driving: Challenges, Possible Solutions and Opportunities},
  author        = {Senkang Hu and Zhengru Fang and Yiqin Deng and Xianhao Chen and Yuguang Fang},
  year          = {2025},
  eprint        = {2401.01544},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2401.01544}
}
@misc{zhu2023tpatchtriggeredphysicaladversarial,
  title         = {TPatch: A Triggered Physical Adversarial Patch},
  author        = {Wenjun Zhu and Xiaoyu Ji and Yushi Cheng and Shibo Zhang and Wenyuan Xu},
  year          = {2023},
  eprint        = {2401.00148},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/2401.00148}
}
@inproceedings{pmlr-v80-athalye18b,
  title     = {Synthesizing Robust Adversarial Examples},
  author    = {Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages     = {284--293},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  month     = {10--15 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v80/athalye18b/athalye18b.pdf},
  url       = {https://proceedings.mlr.press/v80/athalye18b.html},
  abstract  = {Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.}
}
@misc{brown2018adversarialpatch,
  title         = {Adversarial Patch},
  author        = {Tom B. Brown and Dandelion Mané and Aurko Roy and Martín Abadi and Justin Gilmer},
  year          = {2018},
  eprint        = {1712.09665},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1712.09665}
}
@inbook{Chen_2019,
  title     = {ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object Detector},
  isbn      = {9783030109257},
  issn      = {1611-3349},
  url       = {http://dx.doi.org/10.1007/978-3-030-10925-7_4},
  doi       = {10.1007/978-3-030-10925-7_4},
  booktitle = {Machine Learning and Knowledge Discovery in Databases},
  publisher = {Springer International Publishing},
  author    = {Chen, Shang-Tse and Cornelius, Cory and Martin, Jason and Chau, Duen Horng},
  year      = {2019},
  pages     = {52–68}
}
@misc{goodfellow2015explainingharnessingadversarialexamples,
  title         = {Explaining and Harnessing Adversarial Examples},
  author        = {Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
  year          = {2015},
  eprint        = {1412.6572},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1412.6572}
}
@misc{szegedy2014intriguingpropertiesneuralnetworks,
  title         = {Intriguing properties of neural networks},
  author        = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
  year          = {2014},
  eprint        = {1312.6199},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1312.6199}
}
@inproceedings{10.11453319535.3354259,
  author    = {Zhao, Yue and Zhu, Hong and Liang, Ruigang and Shen, Qintao and Zhang, Shengzhi and Chen, Kai},
  title     = {Seeing isn't Believing: Towards More Robust Adversarial Attack Against Real World Object Detectors},
  year      = {2019},
  isbn      = {9781450367479},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3319535.3354259},
  doi       = {10.1145/3319535.3354259},
  abstract  = {Recently Adversarial Examples (AEs) that deceive deep learning models have been a topic of intense research interest. Compared with the AEs in the digital space, the physical adversarial attack is considered as a more severe threat to the applications like face recognition in authentication, objection detection in autonomous driving cars, etc. In particular, deceiving the object detectors practically, is more challenging since the relative position between the object and the detector may keep changing. Existing works attacking object detectors are still very limited in various scenarios, e.g., varying distance and angles, etc. In this paper, we presented systematic solutions to build robust and practical AEs against real world object detectors. Particularly, for Hiding Attack (HA), we proposed thefeature-interference reinforcement (FIR) method and theenhanced realistic constraints generation (ERG) to enhance robustness, and for Appearing Attack (AA), we proposed thenested-AE, which combines two AEs together to attack object detectors in both long and short distance. We also designed diverse styles of AEs to make AA more surreptitious. Evaluation results show that our AEs can attack the state-of-the-art real-time object detectors (i.e., YOLO V3 and faster-RCNN) at the success rate up to 92.4\% with varying distance from 1m to 25m and angles from -60º to 60º. Our AEs are also demonstrated to be highly transferable, capable of attacking another three state-of-the-art black-box models with high success rate.},
  booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {1989–2004},
  numpages  = {16},
  keywords  = {neural networks, object detectors, physical adversarial attack},
  location  = {London, United Kingdom},
  series    = {CCS '19}
}
@inproceedings{10.1007978-3-030-58536-5_36,
  author    = {Wang, Tsun-Hsuan and Manivasagam, Sivabalan and Liang, Ming and Yang, Bin and Zeng, Wenyuan and Urtasun, Raquel},
  title     = {V2VNet: Vehicle-to-Vehicle Communication for Joint Perception and Prediction},
  year      = {2020},
  isbn      = {978-3-030-58535-8},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  url       = {https://doi.org/10.1007/978-3-030-58536-5_36},
  doi       = {10.1007/978-3-030-58536-5_36},
  abstract  = {In this paper, we explore the use of vehicle-to-vehicle (V2V) communication to improve the perception and motion forecasting performance of self-driving vehicles. By intelligently aggregating the information received from multiple nearby vehicles, we can observe the same scene from different viewpoints. This allows us to see through occlusions and detect actors at long range, where the observations are very sparse or non-existent. We also show that our approach of sending compressed deep feature map activations achieves high accuracy while satisfying communication bandwidth requirements.},
  booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II},
  pages     = {605–621},
  numpages  = {17},
  keywords  = {Autonomous driving, Object detection, Motion forecast},
  location  = {Glasgow, United Kingdom}
}
@inproceedings{96294f63afd44ccd9210824e1aa2656c,
  title     = {Adaptive Communications in Collaborative Perception with Domain Alignment for Autonomous Driving},
  abstract  = {Collaborative perception among multiple connected and autonomous vehicles (CAVs) can greatly enhance perceptive capabilities by allowing vehicles to exchange supplementary information. Despite significant advances, many design challenges still remain due to channel variations and data heterogeneity among collaborative vehicles. To address these issues, we propose ACC-DA, a channel-aware collaborative perception framework to dynamically adjust the communication graph to minimize the average transmission delay while mitigating the impacts caused by data heterogeneity. More specifically, we first construct the communication graph to minimize the transmission delay according to different channel information state. We then propose an adaptive data reconstruction mechanism to dynamically adjust the rate-distortion trade-off to enhance perception efficiency while reducing the temporal redundancy during data transmissions. Finally, we conceive a domain alignment scheme to align the data distribution from different vehicles to mitigate the domain gap between different vehicles and improve the performance of the target task. Comprehensive experiments demonstrate the effectiveness of our method in comparison to the existing state-of-the-art works. {\textcopyright} 2024 IEEE.},
  keywords  = {cs.AI, Collaborative Perception, Connected and Autonomous Driving, Channel-Aware, Domain Alignment},
  author    = {Senkang Hu and Zhengru Fang and Haonan An and Guowen Xu and Yuan Zhou and Xianhao Chen and Yuguang Fang},
  year      = {2024},
  month     = dec,
  doi       = {10.1109/GLOBECOM52923.2024.10901656},
  language  = {English},
  isbn      = {9798350351262},
  series    = {Proceedings - IEEE Global Communications Conference, GLOBECOM},
  publisher = {IEEE},
  pages     = {746--751},
  booktitle = {GLOBECOM 2024 - 2024 IEEE Global Communications Conference},
  address   = {United States},
  note      = {2024 IEEE Global Communications Conference (GLOBECOM 2024) : <i>Connecting the Intelligent World through Africa</i>, IEEE GLOBECOM 2024 ; Conference date: 08-12-2024 Through 12-12-2024},
  url       = {https://globecom2024.ieee-globecom.org/}
}
@misc{hu2022where2commcommunicationefficientcollaborativeperception,
  title         = {Where2comm: Communication-Efficient Collaborative Perception via Spatial Confidence Maps},
  author        = {Yue Hu and Shaoheng Fang and Zixing Lei and Yiqi Zhong and Siheng Chen},
  year          = {2022},
  eprint        = {2209.12836},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2209.12836}
}
@inproceedings{6a619c52e2df4c96ac051544a08e266d,
  title     = {Robust Collaborative 3D Object Detection in Presence of Pose Errors},
  abstract  = {Collaborative 3D object detection exploits information exchange among multiple agents to enhance accuracy of object detection in presence of sensor impairments such as occlusion. However, in practice, pose estimation errors due to imperfect localization would cause spatial message misalignment and significantly reduce the performance of collaboration. To alleviate adverse impacts of pose errors, we propose CoAlign, a novel hybrid collaboration framework that is robust to unknown pose errors. The proposed solution relies on a novel agent-object pose graph modeling to enhance pose consistency among collaborating agents. Furthermore, we adopt a multiscale data fusion strategy to aggregate intermediate features at multiple spatial resolutions. Comparing with previous works, which require ground-truth pose for training supervision, our proposed CoAlign is more practical since it doesn't require any ground-truth pose supervision in the training and makes no specific assumptions on pose errors. Extensive evaluation of the proposed method is carried out on multiple datasets, certifying that CoAlign significantly reduce relative localization error and achieving the state of art detection performance when pose errors exist. Code are made available for the use of the research community at https://github.com/yifanlu0227/CoAlign.},
  author    = {Yifan Lu and Quanhao Li and Baoan Liu and Mehrdad Dianati and Chen Feng and Siheng Chen and Yanfeng Wang},
  note      = {Publisher Copyright: {\textcopyright} 2023 IEEE.; 2023 IEEE International Conference on Robotics and Automation, ICRA 2023 ; Conference date: 29-05-2023 Through 02-06-2023},
  year      = {2023},
  doi       = {10.1109/ICRA48891.2023.10160546},
  language  = {English (US)},
  series    = {Proceedings - IEEE International Conference on Robotics and Automation},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  pages     = {4812--4818},
  booktitle = {Proceedings - ICRA 2023}
}
@article{DBLP:journalscorrabs-1712-09665,
  author     = {Tom B. Brown and
                Dandelion Man{\'{e}} and
                Aurko Roy and
                Mart{\'{\i}}n Abadi and
                Justin Gilmer},
  title      = {Adversarial Patch},
  journal    = {CoRR},
  volume     = {abs/1712.09665},
  year       = {2017},
  url        = {http://arxiv.org/abs/1712.09665},
  eprinttype = {arXiv},
  eprint     = {1712.09665},
  timestamp  = {Mon, 13 Aug 2018 16:46:21 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1712-09665.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{10.11453372297.3423359,
  author    = {Nassi, Ben and Mirsky, Yisroel and Nassi, Dudi and Ben-Netanel, Raz and Drokin, Oleg and Elovici, Yuval},
  title     = {Phantom of the ADAS: Securing Advanced Driver-Assistance Systems from Split-Second Phantom Attacks},
  year      = {2020},
  isbn      = {9781450370899},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3372297.3423359},
  doi       = {10.1145/3372297.3423359},
  abstract  = {In this paper, we investigate "split-second phantom attacks," a scientific gap that causes two commercial advanced driver-assistance systems (ADASs), Telsa Model X (HW 2.5 and HW 3) and Mobileye 630, to treat a depthless object that appears for a few milliseconds as a real obstacle/object. We discuss the challenge that split-second phantom attacks create for ADASs. We demonstrate how attackers can apply split-second phantom attacks remotely by embedding phantom road signs into an advertisement presented on a digital billboard which causes Tesla's autopilot to suddenly stop the car in the middle of a road and Mobileye 630 to issue false notifications. We also demonstrate how attackers can use a projector in order to cause Tesla's autopilot to apply the brakes in response to a phantom of a pedestrian that was projected on the road and Mobileye 630 to issue false notifications in response to a projected road sign. To counter this threat, we propose a countermeasure which can determine whether a detected object is a phantom or real using just the camera sensor. The countermeasure (GhostBusters) uses a "committee of experts" approach and combines the results obtained from four lightweight deep convolutional neural networks that assess the authenticity of an object based on the object's light, context, surface, and depth. We demonstrate our countermeasure's effectiveness (it obtains a TPR of 0.994 with an FPR of zero) and test its robustness to adversarial machine learning attacks.},
  booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {293–308},
  numpages  = {16},
  keywords  = {advanced driver-assistance systems, neural-networks, security, split-second phantom attacks},
  location  = {Virtual Event, USA},
  series    = {CCS '20}
}
@misc{zhou2024transientadversarial3dprojection,
  title         = {Transient Adversarial 3D Projection Attacks on Object Detection in Autonomous Driving},
  author        = {Ce Zhou and Qiben Yan and Sijia Liu},
  year          = {2024},
  eprint        = {2409.17403},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/2409.17403}
}
@misc{köhler2021rollininherentvulnerabilityrolling,
  title         = {They See Me Rollin': Inherent Vulnerability of the Rolling Shutter in CMOS Image Sensors},
  author        = {Sebastian Köhler and Giulio Lovisotto and Simon Birnbach and Richard Baker and Ivan Martinovic},
  year          = {2021},
  eprint        = {2101.10011},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2101.10011}
}
@inproceedings{10.11453548606.3559390,
  author    = {Muller, Raymond and Man, Yanmao and Celik, Z. Berkay and Li, Ming and Gerdes, Ryan},
  title     = {Physical Hijacking Attacks against Object Trackers},
  year      = {2022},
  isbn      = {9781450394505},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3548606.3559390},
  doi       = {10.1145/3548606.3559390},
  abstract  = {Modern autonomous systems rely on both object detection and object tracking in their visual perception pipelines. Although many recent works have attacked the object detection component of autonomous vehicles, these attacks do not work on full pipelines that integrate object tracking to enhance the object detector's accuracy. Meanwhile, existing attacks against object tracking either lack real-world applicability or do not work against a powerful class of object trackers, Siamese trackers. In this paper, we present AttrackZone, a new physically-realizable tracker hijacking attack against Siamese trackers that systematically determines valid regions in an environment that can be used for physical perturbations. AttrackZone exploits the heatmap generation process of Siamese Region Proposal Networks in order to take control of an object's bounding box, resulting in physical consequences including vehicle collisions and masked intrusion of pedestrians into unauthorized areas. Evaluations in both the digital and physical domain show that AttrackZone achieves its attack goals 92\% of the time, requiring only 0.3-3 seconds on average.},
  booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {2309–2322},
  numpages  = {14},
  keywords  = {video surveillance, object tracking, neural networks, autonomous driving, adversarial machine learning},
  location  = {Los Angeles, CA, USA},
  series    = {CCS '22}
}
@inproceedings{277268,
  author    = {Chen Yan and Zhijian Xu and Zhanyuan Yin and Xiaoyu Ji and Wenyuan Xu},
  title     = {Rolling Colors: Adversarial Laser Exploits against Traffic Light Recognition},
  booktitle = {31st USENIX Security Symposium (USENIX Security 22)},
  year      = {2022},
  isbn      = {978-1-939133-31-1},
  address   = {Boston, MA},
  pages     = {1957--1974},
  url       = {https://www.usenix.org/conference/usenixsecurity22/presentation/yan},
  publisher = {USENIX Association},
  month     = aug
}
@dataset{DAIR-V2X2021,
title={Vehicle-Infrastructure Collaborative Autonomous Driving: DAIR-V2X Dataset},
author={Institue for AI Industry Research (AIR), Tsinghua University},
website={http://air.tsinghua.edu.cn/dair-v2x},
year={2021}
}
@article{nuscenes2019,
  title={nuScenes: A multimodal dataset for autonomous driving},
  author={Holger Caesar and Varun Bankiti and Alex H. Lang and Sourabh Vora and 
          Venice Erin Liong and Qiang Xu and Anush Krishnan and Yu Pan and 
          Giancarlo Baldan and Oscar Beijbom},
  journal={arXiv preprint arXiv:1903.11027},
  year={2019}
}
@misc{xu2022opv2vopenbenchmarkdataset,
      title={OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication}, 
      author={Runsheng Xu and Hao Xiang and Xin Xia and Xu Han and Jinlong Li and Jiaqi Ma},
      year={2022},
      eprint={2109.07644},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2109.07644}, 
}
@misc{xu2022cobevtcooperativebirdseye,
      title={CoBEVT: Cooperative Bird's Eye View Semantic Segmentation with Sparse Transformers}, 
      author={Runsheng Xu and Zhengzhong Tu and Hao Xiang and Wei Shao and Bolei Zhou and Jiaqi Ma},
      year={2022},
      eprint={2207.02202},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2207.02202}, 
}
@inproceedings{yu2024_univ2x,
 title={End-to-End Autonomous Driving through V2X Cooperation}, 
 author={Haibao Yu and Wenxian Yang and Jiaru Zhong and Zhenwei Yang and Siqi Fan and Ping Luo and Zaiqing Nie},
 booktitle={The 39th Annual AAAI Conference on Artificial Intelligence},
 year={2025}
}